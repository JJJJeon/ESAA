{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JJJJeon/ESAA/blob/main/5%EC%A3%BC%EC%B0%A81.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 텍스트분석"
      ],
      "metadata": {
        "id": "gzkScevUCI9k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "비정형 데이터인 텍스트를 분석하는 것\n",
        "\n",
        "* 피처 벡터화 또는 피처 추출: 텍스트 변환"
      ],
      "metadata": {
        "id": "uDu6L0VKA2L-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text_sample = 'The Matrix is every'\n",
        "\n",
        "text_sample='The Matrix is everywhere its all around us, here even in this room. \\nYou can see it out your window or on your television. \\nYou feel it when you go to work, or go to church or pay your taxes.'\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "\n",
        "print(type(sentences), len(sentences))\n",
        "print(sentences)\n",
        "\n",
        "## 리스트 타입"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9At866iM0vo",
        "outputId": "ec000aad-220b-42ec-a62e-adc2c0465234"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = \"The matrix is everywhere its all around us, here even in this room.\"\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)\n",
        "\n",
        "## 리스트 타입\n",
        "## 마침표(.)도 단어에 포함"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nFRqsETjM0yo",
        "outputId": "3ff668c6-fac6-4fa4-d2a5-4a4fdd055dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "# 여러 개의 문장으로 된 입력 데이터를 문장별로 단어 토큰화하게 만드는 함수 생성\n",
        "def tokenize_text(text):\n",
        "  # 문장을 분리\n",
        "  sentences = sent_tokenize(text)\n",
        "  # 분리된 문장별 단어 토큰화\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokens\n",
        "\n",
        "# 여러 문장에 대해 문장별 단어 \n",
        "word_tokens = tokenize_text(text_sample)\n",
        "\n",
        "print(type(word_tokens), len(word_tokens))\n",
        "print(word_tokens)\n",
        "\n",
        "## 리스트 안에 3개의 리스트 "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvcFyFe8M02K",
        "outputId": "3669a6c6-67cf-4c40-a54b-75c84d7fd0c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk \n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tEDw4mWtXzXM",
        "outputId": "35ec0e04-5926-41f9-fdff-0324d65ee5d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('영어 stop words 개수:', len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wdDlieOFXzaI",
        "outputId": "2bf2c88c-8291-4940-e9da-c4ea6e62e012"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 stop words 개수: 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "\n",
        "# 위 예제에서 3개의 문장별로 얻은 word_tokens list에 대해 스톱워드를 제거하는 반복문\n",
        "for sentence in word_tokens:\n",
        "  filtered_words=[]\n",
        "  # 개별 문장별로 토큰화된 문장 list에 대해 스톱워드를 제거하는 반복문\n",
        "  for word in sentence:\n",
        "    # 소문자로 모두 변환\n",
        "    word=word.lower()\n",
        "    # 토큰화된 개별단어가 스톱워드에 포함되지 않으면 word_tokens에 추가\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "  all_tokens.append(filtered_words)\n",
        "\n",
        "print(all_tokens) #us는 잇다다\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFBinHKQXzdq",
        "outputId": "eded7fc8-0901-4457-b5eb-77d1a527cc3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'), stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))\n",
        "\n",
        "## 덜 정교"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd0J8carhlx5",
        "outputId": "8180671e-81f3-4506-de52-5f980a38577e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus amus\n",
            "happy happiest\n",
            "fant fanciest\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('amusing', 'v'), lemma.lemmatize('amuses', 'v'), lemma.lemmatize('amused', 'v'))\n",
        "print(lemma.lemmatize('happier', 'a'), lemma.lemmatize('happiest', 'a'))\n",
        "print(lemma.lemmatize('fancier', 'a'), lemma.lemmatize('fanciest', 'a'))\n",
        "\n",
        "## 더 정교교"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MJAkEIjhl4O",
        "outputId": "a5c4a65b-8c9f-4653-82f1-691cb5a4b9a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BOW (Bag of Words)"
      ],
      "metadata": {
        "id": "qIScrwT8kNoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "dense = np.array([[3,0,1],\n",
        "                  [0,2,0]])\n",
        "\n",
        "# 위 밀집 행렬을 사이파이의 coo_matrix 클래스를 이용해 \n",
        "# COO형식의 희소행렬로 변환\n",
        "from scipy import sparse\n",
        "\n",
        "# 1. 0이 아닌 데이터 추출\n",
        "data = np.array([3,1,2])\n",
        "\n",
        "# 2. 행위치와 열위치를 각각 배열로 생성\n",
        "row_pos = np.array([0,0,1])\n",
        "col_pos = np.array([0,2,1])\n",
        "## 3:(0,0), 1:(0,2), 2:(1,1)\n",
        "\n",
        "# 3. sparse 패키지의 coo_matrix를 이용해 COO형식으로 희소행렬 생성\n",
        "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos)))"
      ],
      "metadata": {
        "id": "QwHH_ybwkhm3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_coo.toarray()\n",
        "## 확인"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJjkxa8mzmS_",
        "outputId": "9d91e98d-baea-4a63-928d-5a2f26bea91f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "\n",
        "dense2=np.array([[0,0,1,0,0,5],\n",
        "                 [1,4,0,3,2,5],\n",
        "                 [0,6,0,3,0,0],\n",
        "                 [2,0,0,0,0,0],\n",
        "                 [0,0,0,7,0,8],\n",
        "                 [1,0,0,0,0,0]])\n",
        "\n",
        "#0이 아닌 데이터 추출\n",
        "data2=np.array([1,5,1,4,3,2,5,6,3,2,7,8,1])\n",
        "\n",
        "#행 위치와 열 위치를 각각 array로 생성\n",
        "row_pos=np.array([0,0,1,1,1,1,1,2,2,3,4,4,5])\n",
        "col_pos=np.array([2,5,0,1,3,4,5,1,3,0,3,5,0])\n",
        "\n",
        "#COO 형식으로 변환\n",
        "sparse_coo=sparse.coo_matrix((data2, (row_pos, col_pos)))\n",
        "\n",
        "#행 위치 배열의 고유한 값의 시작 위치 인덱스를 배열로 생성 + 맨 끝에는 총 데이터 수\n",
        "row_pos_ind=np.array([0,2,7,9,10,12,13])\n",
        "\n",
        "#CSR 형식으로 변환\n",
        "sparse_csr=sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
        "\n",
        "print('COO 변환된 데이터가 제대로 되었는지 다시 dense로 출력 확인')\n",
        "print(sparse_coo.toarray())\n",
        "print('CSR 변환된 데이터가 제대로 되었는지 다시 dense로 출력 확인')\n",
        "print(sparse_csr.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ybWTXj5vzmVi",
        "outputId": "51f6187d-ca4f-47f3-adbc-fe7685cdebb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COO 변환된 데이터가 제대로 되었는지 다시 dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "CSR 변환된 데이터가 제대로 되었는지 다시 dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 실제 사용시에는 바로 밀집행렬을 input을 넣으면 됨\n",
        "\n",
        "\n",
        "dense3=np.array([[0,0,1,0,0,5],\n",
        "                 [1,4,0,3,2,5],\n",
        "                 [0,6,0,3,0,0],\n",
        "                 [2,0,0,0,0,0],\n",
        "                 [0,0,0,7,0,8],\n",
        "                 [1,0,0,0,0,0]])\n",
        "\n",
        "coo=sparse.coo_matrix(dense3)\n",
        "csr=sparse.csr_matrix(dense3)\n",
        "\n",
        "print(coo.toarray())\n",
        "print(csr.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TkP-RRBxzmXL",
        "outputId": "3172b386-8321-4bac-f1b0-11ca5f0cc9e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ]
    }
  ]
}